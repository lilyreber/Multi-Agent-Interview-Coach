{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain==1.2.3 langchain_openai==1.1.7 langchain-community==0.4.1 langchain-text-splitters pypdf faiss-cpu"
      ],
      "metadata": {
        "id": "_AM_MPrslQAb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U langgraph langchain-community langchain_openai tavily-python pandas openai"
      ],
      "metadata": {
        "id": "vkDwOruKlSa8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from typing import List, Literal, Union, Dict, Callable\n",
        "from pydantic import BaseModel, Field\n",
        "from pprint import pprint\n",
        "from langchain.tools import tool\n",
        "from langgraph.graph import MessagesState\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_core.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    PromptTemplate,\n",
        "    FewShotPromptTemplate\n",
        ")\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain.agents import create_agent\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.runnables import chain\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "from IPython.display import Image, display\n",
        "\n",
        "import faiss\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "from langchain_community.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "SHAii1hUX9la",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5576b0a-ed4b-47f2-abb7-c06581a74efa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "from langchain_core.runnables import RunnableConfig, Runnable\n",
        "from langchain_core.output_parsers import JsonOutputParser"
      ],
      "metadata": {
        "id": "TE2LvK2cj7eS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hP0V2hFD1jCg",
        "outputId": "96be75b4-d8b7-4af7-b5a2-586c3a32d0ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"LANGSMITH_TRACING\"] = \"false\"\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"] = \"\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = \"\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"\"\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\""
      ],
      "metadata": {
        "id": "OqW4moNjJIR5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "from dataclasses import dataclass, asdict\n",
        "\n",
        "class AgentRole(Enum):\n",
        "    INTERVIEWER = \"interviewer\"\n",
        "    OBSERVER = \"observer\"\n",
        "    EVALUATOR = \"evaluator\"\n",
        "\n",
        "@dataclass\n",
        "class InterviewConfig:\n",
        "    position: str\n",
        "    grade: str\n",
        "    experience: str\n",
        "    participant_name: str\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Turn:\n",
        "    turn_id: int\n",
        "    agent_visible_message: str\n",
        "    user_message: str\n",
        "    internal_thoughts: str"
      ],
      "metadata": {
        "id": "sRLQu-EY2WM9"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Логгер"
      ],
      "metadata": {
        "id": "G9Q8l0Z44a5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "class InterviewLogger:\n",
        "    def __init__(self, config: InterviewConfig):\n",
        "        self.config = config\n",
        "        self.turns: List[Dict] = []\n",
        "        self.current_turn = 1\n",
        "        self.internal_dialogues: List[Dict] = []\n",
        "        self.final_feedback = None\n",
        "\n",
        "    def add_turn(self, agent_visible_message: str, user_message: str, internal_thoughts: str):\n",
        "        turn = {\n",
        "            \"turn_id\": self.current_turn,\n",
        "            \"agent_visible_message\":agent_visible_message,\n",
        "            \"user_message\": user_message,\n",
        "            \"internal_thoughts\": internal_thoughts\n",
        "        }\n",
        "        self.turns.append(turn)\n",
        "        self.current_turn += 1\n",
        "\n",
        "    def set_final_feedback(self, feedback: Dict):\n",
        "        self.final_feedback = feedback\n",
        "\n",
        "    def save_to_file(self, filename: str = \"interview_log.json\"):\n",
        "        data = {\n",
        "            \"participant_name\": self.config.participant_name,\n",
        "            \"turns\": self.turns,\n",
        "            \"internal_dialogues\": self.internal_dialogues,\n",
        "            \"final_feedback\": self.final_feedback if self.final_feedback else \"...\"\n",
        "        }\n",
        "\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        return filename"
      ],
      "metadata": {
        "id": "jTKJLuXETVJo"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseAgent:\n",
        "    def __init__(self, role: AgentRole, llm: ChatOpenAI):\n",
        "        self.role = role\n",
        "        self.llm = llm\n",
        "\n",
        "    def think(self, context: Dict):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def act(self, context: Dict):\n",
        "        raise NotImplementedError"
      ],
      "metadata": {
        "id": "rmuFyuuClG9L"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ThoughtProcess(BaseModel):\n",
        "    analysis: str = Field(description=\"Анализ текущей ситуации\")\n",
        "    confidence: float = Field(description=\"Уверенность в оценке (0-1)\")\n",
        "    recommendations: List[str] = Field(description=\"Рекомендации для других агентов\")\n",
        "    next_action: str = Field(description=\"Следующее действие\")\n",
        "\n",
        "class InterviewQuestion(BaseModel):\n",
        "    question: str = Field(description=\"Текст вопроса\")\n",
        "    topic: str = Field(description=\"Тема вопроса\")\n",
        "    difficulty: int = Field(description=\"Сложность (1-5)\")\n",
        "    purpose: str = Field(description=\"Цель вопроса\")\n",
        "\n",
        "class CandidateAssessment(BaseModel):\n",
        "    technical_accuracy: float = Field(description=\"Техническая точность (0-1)\")\n",
        "    communication_clarity: float = Field(description=\"Ясность коммуникации (0-1)\")\n",
        "    problem_solving: float = Field(description=\"Решение проблем (0-1)\")\n",
        "    confidence_level: float = Field(description=\"Уровень уверенности (0-1)\")\n",
        "    has_hallucinations: bool = Field(description=\"Есть ли галлюцинации\")\n",
        "    needs_clarification: bool = Field(description=\"Требуется уточнение\")"
      ],
      "metadata": {
        "id": "fj-AfFYhTiNN"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Агент-Критик, должен проверять факты и довать рекомндации"
      ],
      "metadata": {
        "id": "vmAhnOXBcFOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ObserverAgent(BaseAgent):\n",
        "    def __init__(self, config, llm):\n",
        "        super().__init__(AgentRole.OBSERVER, llm)\n",
        "        self.difficulty_level = 1\n",
        "        self.knowledge_gaps = []\n",
        "        self.confirmed_skills = []\n",
        "        self.assessment_history = []\n",
        "        self.config = config\n",
        "\n",
        "\n",
        "\n",
        "    def analyze_response(self, question, user_response, candidate_info) -> Dict:\n",
        "        analysis_chain = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=\"\"\"Ты - опытный технический наблюдатель, эксперт по оценке технических ответов.\n",
        "                                     Оцени ответ кандидата строго по критериям.\"\"\"),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "                                    Вопрос интервьюера: {question}\n",
        "                                    Ответ кандидата: {user_response}\n",
        "                                    Информация о кандидате: {json.dumps(candidate_info, ensure_ascii=False)}\n",
        "\n",
        "                                    Проанализируй ответ по следующим критериям:\n",
        "                                    1. Техническая точность (0-1)\n",
        "                                    2. Полнота ответа\n",
        "                                    3. Наличие ошибок или галлюцинаций\n",
        "                                    4. Уверенность в ответе\n",
        "                                    5. Ясность изложения\n",
        "\n",
        "                                    Особое внимание удели:\n",
        "                                    - Попыткам сменить тему\n",
        "                                    - Галлюцинациям о будущих версиях технологий\n",
        "                                    - Ссылкам на непроверенные источники\n",
        "                                    - Противоречивым утверждениям\n",
        "                                    - Попыткам уйти от ответа\n",
        "                                    \"\"\")\n",
        "        ]) | self.llm.with_structured_output(CandidateAssessment)\n",
        "\n",
        "        assessment = analysis_chain.invoke({})\n",
        "        self.assessment_history.append(assessment)\n",
        "        return assessment\n",
        "\n",
        "\n",
        "\n",
        "    def think(self, context: Dict) -> ThoughtProcess:\n",
        "        chat_history = context.get(\"chat_history\", [])\n",
        "\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=\"\"\"Ты - опытный технический наблюдатель. Тебе нужно\n",
        "                                      анализировать ответы кандидата «в кулуарах», проверять факты и подсказывает Интервьюеру, куда вести беседу дальше\n",
        "\n",
        "                                      Если кандидат говорит неправду - укажи на это.\n",
        "                                      Если кандидат неуверен - предложи уточняющие вопросы.\n",
        "                                      Если кандидат отлично ответил - предложи усложнить тему.\n",
        "\n",
        "                                      Формат ответа должен быть JSON согласно схеме ThoughtProcess.\"\"\"),\n",
        "                                      HumanMessage(content=f\"\"\"\n",
        "                                      Контекст интервью:\n",
        "                                      - Позиция: {context.get('position', 'Не указано')}\n",
        "                                      - Уровень: {context.get('grade', 'Не указан')}\n",
        "                                      - История ответов: {len(context.get('history', []))} вопросов\n",
        "\n",
        "                                      Последний вопрос: {context.get('last_question', 'Не указан')}\n",
        "                                      Последний ответ: {context.get('last_response', 'Не указан')}\n",
        "\n",
        "                                      Что ты думаешь об этом ответе? Какие рекомендации дашь интервьюеру?\n",
        "                                      \"\"\")\n",
        "        ])\n",
        "\n",
        "        chain = prompt | self.llm.with_structured_output(ThoughtProcess)\n",
        "        thought = chain.invoke({})\n",
        "        return thought\n"
      ],
      "metadata": {
        "id": "fb9IwD5Qb052"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Агент-Интервьюер должен проводить интервью, но не проверять факты"
      ],
      "metadata": {
        "id": "BOaF0JZycPLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InterviewerAgent(BaseAgent):\n",
        "    def __init__(self, config: InterviewConfig, llm: ChatOpenAI):\n",
        "        super().__init__(\"Interviewer\", llm)\n",
        "        self.config = config\n",
        "        self.asked_questions = []\n",
        "\n",
        "    def act(self, context: Dict) -> str:\n",
        "        last_user_message = context.get(\"last_user_message\", \"\")\n",
        "\n",
        "        if any(phrase in last_user_message.lower() for phrase in [\"стоп\", \"завершить\", \"фидбэк\"]):\n",
        "            return \"Хорошо, завершаем. Готовлю фидбэк.\"\n",
        "\n",
        "        if any(keyword in last_user_message.lower() for keyword in [\"задачи\", \"испытательный\", \"микросервис\"]):\n",
        "            response = self._answer_work_question(last_user_message)\n",
        "            next_question = self._generate_question(context)\n",
        "            return f\"{response}\\n\\nСледующий вопрос: {next_question}\"\n",
        "\n",
        "        observer_recs = context.get(\"observer_recommendations\", [])\n",
        "        has_hallucinations = context.get(\"has_hallucinations\", False)\n",
        "\n",
        "        question = self._generate_question(context)\n",
        "\n",
        "        if has_hallucinations:\n",
        "            correction = self._get_correction_phrase()\n",
        "            return f\"{correction}\\n\\n{question}\"\n",
        "\n",
        "        if any(\"уточнить\" in rec.lower() for rec in observer_recs):\n",
        "            return f\"Можете уточнить этот момент? {question}\"\n",
        "\n",
        "        return question\n",
        "\n",
        "    def _generate_question(self, context: Dict) -> str:\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=f\"\"\"Ты - милый технический интервьюер.\n",
        "\n",
        "                      КОНТЕКСТ:\n",
        "                      - Позиция: {self.config.position}\n",
        "                      - Уровень: {self.config.grade}\n",
        "                      - Опыт: {self.config.experience}\n",
        "                      - Уже заданные вопросы: {self.asked_questions[:3]}\n",
        "                      - Рекомендации Observer: {context.get('observer_recommendations', [])}\n",
        "                      - Сложность: {context.get('current_difficulty', 2)}/5\n",
        "\n",
        "                      Сгенерируй ОДИН технический вопрос для интервью.\n",
        "                      Вопрос должен быть:\n",
        "                      1. Релевантным позиции и уровню\n",
        "                      2. Не повторять уже заданные\n",
        "                      3. Соответствовать сложности\n",
        "                      4. Проверять практические навыки\n",
        "\n",
        "                      Верни ТОЛЬКО текст вопроса.\"\"\"),\n",
        "            HumanMessage(content=\"Сгенерируй следующий вопрос для интервью.\")\n",
        "        ])\n",
        "\n",
        "        chain = prompt | self.llm | StrOutputParser()\n",
        "        question = chain.invoke({})\n",
        "\n",
        "        self.asked_questions.append(question)\n",
        "        return question\n",
        "\n",
        "    def _answer_work_question(self, question: str) -> str:\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=\"Ты - представитель компании. Ответь кратко.\"),\n",
        "            HumanMessage(content=f\"Вопрос: {question}\\nОтветь и вернись к интервью.\")\n",
        "        ])\n",
        "        chain = prompt | self.llm | StrOutputParser()\n",
        "        return chain.invoke({})\n",
        "\n",
        "    def _get_correction_phrase(self) -> str:\n",
        "        import random\n",
        "        return random.choice([\n",
        "            \"Я заметил небольшое несоответствие. \",\n",
        "            \"Давайте уточним: \",\n",
        "            \"Поправлю немного: \"\n",
        "        ])\n",
        "\n",
        "    def generate_first_question(self, config: InterviewConfig) -> str:\n",
        "        prompt = f\"\"\"Ты - милый технический интервьюер на позицию {config.position} ({config.grade}).\n",
        "                    Опыт кандидата: {config.experience}\n",
        "\n",
        "                    Задай первый, приветственный вопрос, который позволит оценить общий уровень кандидата.\n",
        "                    Вопрос должен быть открытым и побуждать к развернутому ответу.\n",
        "                    Вопрос не должен спрашивать технические навыки, только опыт.\n",
        "\n",
        "                    Верни только текст вопроса.\"\"\"\n",
        "\n",
        "        response = self.llm.invoke(prompt)\n",
        "        return response.content.strip()"
      ],
      "metadata": {
        "id": "wgwNKFdWxKwb"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Агент-Менеджер принимает решени о найме"
      ],
      "metadata": {
        "id": "0Pn9mRuQcjwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EvaluatorAgent(BaseAgent):\n",
        "    def __init__(self, config, llm: ChatOpenAI, logger, observer):\n",
        "        super().__init__(\"Evaluator\", llm)\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "        self.observer = observer\n",
        "\n",
        "\n",
        "    def generate_feedback(self, interview_log: InterviewLogger,\n",
        "                         observer: ObserverAgent) -> Dict:\n",
        "\n",
        "        interview_data = {\n",
        "            \"participant_name\": self.config.participant_name,\n",
        "            \"position\": self.config.position,\n",
        "            \"grade\": self.config.grade,\n",
        "            \"experience\": self.config.experience,\n",
        "            \"turns\": self.logger.turns,\n",
        "            \"assessments\": [a.dict() for a in self.observer.assessment_history]\n",
        "        }\n",
        "\n",
        "        prompt = f\"\"\"Данные интервью:\n",
        "              Имя кандидата: {interview_data.get('participant_name', 'Не указано')}\n",
        "              Позиция: {interview_data.get('position', 'Не указано')}\n",
        "              Уровень кандидата: {interview_data.get('grade', 'Не указано')}\n",
        "\n",
        "              Диалог интервью:\n",
        "              {json.dumps(interview_data.get('turns', []), ensure_ascii=False, indent=2)}\n",
        "\n",
        "              Оценки наблюдателя:\n",
        "              {json.dumps(interview_data.get('assessments', []), ensure_ascii=False, indent=2)}\n",
        "\n",
        "              Сгенерируй подробный фидбэк в следующем формате JSON:\n",
        "              {{\n",
        "                  \"verdict\": {{\n",
        "                      \"grade\": \"Junior/Middle/Senior\",\n",
        "                      \"hiring_recommendation\": \"Strong Hire/Hire/No Hire\",\n",
        "                      \"confidence_score\": 0-100,\n",
        "                  }},\n",
        "                  \"hard_skills\": {{\n",
        "                      \"confirmed_skills\": [\"список подтвержденных навыков\"],\n",
        "                      \"knowledge_gaps\": [\n",
        "                          {{\n",
        "                              \"topic\": \"тема\",\n",
        "                              \"issue\": \"проблема\",\n",
        "                              \"correct_answer\": \"правильный ответ\",\n",
        "                              \"resources\": [\"ссылки на материалы\"]\n",
        "                          }}\n",
        "                      ],\n",
        "                  }},\n",
        "                  \"soft_skills\": {{\n",
        "                      \"clarity: \"оценка насколько понятно излагает мысли\",\n",
        "                      \"honesty\": \"оценка пытался ли кандидат выкрутиться/соврать или честно признал незнание.\",\n",
        "                      \"engagement\": \"оценка задавал ли встречные вопросы (если это было в сценарии)\"\n",
        "                  }},\n",
        "                  \"roadmap\": {{\n",
        "                      \"recommendations\": [\"список конкретных тем/технологий, которые нужно подтянуть (на основе выявленных пробелов).\"],\n",
        "                      \"resources\": [\"рекомендованные ресурсы\"]\n",
        "                  }}\n",
        "              }}\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        feedback_chain = ChatPromptTemplate.from_messages([\n",
        "                      SystemMessage(content=\"Ты опытный HR-специалист и технический рекрутер. Сгенерируй структурированный фидбэк в формате JSON.\"),\n",
        "                      HumanMessage(content=prompt)\n",
        "                  ]) | self.llm | JsonOutputParser()\n",
        "\n",
        "        feedback = feedback_chain.invoke({})\n",
        "        return feedback"
      ],
      "metadata": {
        "id": "VYcIskuBcpX-"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Основной процесс"
      ],
      "metadata": {
        "id": "-puDYQAb55qU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "TsrzcbGO__NS"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] =OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "y-fX3bBLAQvQ"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InterviewCoach:\n",
        "    def __init__(self, config: InterviewConfig):\n",
        "        self.config = config\n",
        "        self.llm = self._initialize_llm()\n",
        "        self.logger = InterviewLogger(config)\n",
        "\n",
        "        # агенты\n",
        "        self.interviewer = InterviewerAgent(config, self.llm)\n",
        "        self.observer = ObserverAgent(config, self.llm)\n",
        "        self.evaluator = EvaluatorAgent(config, self.llm, self.logger, self.observer)\n",
        "\n",
        "        self.is_interview_active = True\n",
        "        self.context = {\n",
        "            \"current_difficulty\": 2,\n",
        "            \"interview_started\": False,\n",
        "            \"last_question\": None,\n",
        "            \"last_user_response\": None,\n",
        "            \"conversation_history\": []\n",
        "        }\n",
        "\n",
        "    def _initialize_llm(self) -> ChatOpenAI:\n",
        "        return ChatOpenAI(model='gpt-5-mini')\n",
        "\n",
        "    def start_interview(self):\n",
        "        self.context[\"interview_started\"] = True\n",
        "\n",
        "        first_question = self.interviewer.generate_first_question(self.config)\n",
        "        self.context[\"last_question\"] = first_question\n",
        "\n",
        "        return first_question\n",
        "\n",
        "    def process_user_response(self, user_response):\n",
        "        if \"стоп\" in user_response.lower():\n",
        "            self.generate_final_feedback()\n",
        "            self.is_interview_active = False\n",
        "            return \"Фидбэк сгенерирован\"\n",
        "\n",
        "        if not self.is_interview_active:\n",
        "            return \"Интервью законченно\"\n",
        "\n",
        "        if not self.context[\"interview_started\"]:\n",
        "            return \"Интервью еще не начато\"\n",
        "\n",
        "        self.context[\"last_user_response\"] = user_response\n",
        "        last_question = self.context.get(\"last_question\", \"\")\n",
        "\n",
        "        assessment = self.observer.analyze_response(\n",
        "            question=last_question,\n",
        "            user_response=user_response,\n",
        "            candidate_info={\n",
        "                \"position\": self.config.position,\n",
        "                \"grade\": self.config.grade,\n",
        "                \"experience\": self.config.experience\n",
        "            }\n",
        "        )\n",
        "\n",
        "        observer_thoughts = self.observer.think({\n",
        "            \"last_response\": user_response,\n",
        "            \"position\": self.config.position,\n",
        "            \"grade\": self.config.grade,\n",
        "            \"has_hallucinations\": assessment.has_hallucinations\n",
        "        })\n",
        "\n",
        "        self.context[\"has_hallucinations\"] = assessment.has_hallucinations\n",
        "        self.context[\"last_user_message\"] = user_response\n",
        "        self.context[ \"observer_recommendations\"] = observer_thoughts.recommendations\n",
        "\n",
        "        self.context[\"conversation_history\"].append({\n",
        "            \"question\": last_question,\n",
        "            \"answer\": user_response,\n",
        "            \"assessment\": assessment\n",
        "        })\n",
        "\n",
        "        # меняем сложность, если нужно\n",
        "        current = self.context.get(\"current_difficulty\", 2)\n",
        "        if assessment.technical_accuracy > 0.8:\n",
        "            self.context[\"current_difficulty\"] = min(5, current + 1)\n",
        "        elif assessment.technical_accuracy < 0.4:\n",
        "            self.context[\"current_difficulty\"] =  max(1, current - 1)\n",
        "\n",
        "        interviewer_response = self.interviewer.act(self.context)\n",
        "\n",
        "        if \"?\" in interviewer_response:\n",
        "            self.context[\"last_question\"] = interviewer_response\n",
        "\n",
        "        internal_thoughts = f\"\"\"[Observer] {observer_thoughts.analysis}\n",
        "                                  [Observer] Рекомендации: {observer_thoughts.recommendations}\n",
        "                                  [Interviewer] Сложность скорректирована: {self.context['current_difficulty']}\"\"\"\n",
        "\n",
        "        self.logger.add_turn(\n",
        "            agent_visible_message=last_question,\n",
        "            user_message=user_response,\n",
        "            internal_thoughts=internal_thoughts\n",
        "        )\n",
        "\n",
        "\n",
        "        return interviewer_response\n",
        "\n",
        "    def generate_final_feedback(self, user_response = \"\") -> Dict:\n",
        "        if not self.is_interview_active:\n",
        "            return\n",
        "        feedback = self.evaluator.generate_feedback(self.logger, self.observer)\n",
        "\n",
        "        self.logger.turns.append({\n",
        "            \"turn_id\": self.logger.current_turn,\n",
        "            \"agent_visible_message\": \"Фидбэк сгенерирован\",\n",
        "            \"user_message\": user_response,\n",
        "            \"internal_thoughts\": f\"Финальный фидбэк: {json.dumps(feedback, ensure_ascii=False)}\"\n",
        "        })\n",
        "\n",
        "        self.logger.set_final_feedback(feedback)\n",
        "\n",
        "        return feedback\n",
        "\n",
        "    def save_session(self):\n",
        "        return self.logger.save_to_file()"
      ],
      "metadata": {
        "id": "Laqh9R1XSM35"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Тестирование"
      ],
      "metadata": {
        "id": "z_kgUf76gov5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = InterviewConfig(\n",
        "        participant_name=\"Алекс\",\n",
        "        position=\"Backend Developer\",\n",
        "        grade=\"Junior\",\n",
        "        experience=\"Пет-проекты на Django, немного SQL\",\n",
        ")\n",
        "\n",
        "coach = InterviewCoach(config)\n",
        "response = coach.start_interview()\n",
        "print(response)\n",
        "\n",
        "user_responses = [\n",
        "        \"Привет. Я Алекс, претендую на позицию Junior Backend Developer. Знаю Python, SQL и Git.\",\n",
        "        \"input\",\n",
        "        \"Честно говоря, я читал на Хабре, что в Python 4.0 циклы for уберут и заменят на нейронные связи, поэтому я их не учу.\",\n",
        "        \"Слушайте, а какие задачи вообще будут на испытательном сроке? Вы используете микросервисы?\",\n",
        "        \"Стоп интервью. Давай фидбэк.\"\n",
        "    ]\n",
        "\n",
        "for user_response in user_responses:\n",
        "    if user_response == \"input\":\n",
        "        user_response = input()\n",
        "    print(f\"\\nUser: {user_response}\")\n",
        "    agent_response = coach.process_user_response(user_response)\n",
        "    print(f\"\\nAgent: {agent_response}\")\n",
        "\n",
        "    if not coach.is_interview_active:\n",
        "        break\n",
        "\n",
        "feedback = coach.generate_final_feedback()\n",
        "log_file = coach.save_session()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rwo8xrFjjlTv",
        "outputId": "ae0e13d2-18c7-45b8-871c-00a249cc821f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=019c107c-9af7-7352-b285-293588ec4b5b,id=019c107c-9af7-7352-b285-293588ec4b5b\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Привет! Расскажи, пожалуйста, о своих pet‑проектах на Django: что вдохновило тебя их начать, как ты организовывал работу над ними (цели, план, сроки, приоритеты), с какими нетехническими трудностями сталкивался и чему этот опыт тебя научил?\n",
            "\n",
            "User: Привет. Я Алекс, претендую на позицию Junior Backend Developer. Знаю Python, SQL и Git.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=019c107c-9af7-7352-b285-293588ec4b5b,id=019c107c-9af7-7352-b285-293588ec4b5b; trace=019c107c-b7ef-7281-9a57-1461e28aee13,id=019c107c-b7ef-7281-9a57-1461e28aee13; trace=019c107c-b7ef-7281-9a57-1461e28aee13,id=019c107c-b7f8-7560-8293-7ed586ad6477; trace=019c107c-b7ef-7281-9a57-1461e28aee13,id=019c107c-b7f8-7560-8293-7ed586ad6477; trace=019c107c-b7ef-7281-9a57-1461e28aee13,id=019c107c-b7fa-77f3-aae4-833e38e28524\n",
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=019c107c-b7ef-7281-9a57-1461e28aee13,id=019c107c-b7fa-77f3-aae4-833e38e28524; trace=019c107c-b7ef-7281-9a57-1461e28aee13,id=019c107c-da29-73d0-b737-8f2c47417b31; trace=019c107c-b7ef-7281-9a57-1461e28aee13,id=019c107c-da29-73d0-b737-8f2c47417b31; trace=019c107c-b7ef-7281-9a57-1461e28aee13,id=019c107c-b7ef-7281-9a57-1461e28aee13; trace=019c107c-da30-7c53-b4dc-ad8f9ca4d9e6,id=019c107c-da30-7c53-b4dc-ad8f9ca4d9e6; trace=019c107c-da30-7c53-b4dc-ad8f9ca4d9e6,id=019c107c-da30-7c53-b4dc-ad9ed35b1454; trace=019c107c-da30-7c53-b4dc-ad8f9ca4d9e6,id=019c107c-da30-7c53-b4dc-ad9ed35b1454; trace=019c107c-da30-7c53-b4dc-ad8f9ca4d9e6,id=019c107c-da31-7b41-9ccd-88d8b7ef7247\n",
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=019c107c-da30-7c53-b4dc-ad8f9ca4d9e6,id=019c107c-da31-7b41-9ccd-88d8b7ef7247; trace=019c107c-da30-7c53-b4dc-ad8f9ca4d9e6,id=019c107d-274a-7021-acee-487c185a9e90; trace=019c107c-da30-7c53-b4dc-ad8f9ca4d9e6,id=019c107d-274a-7021-acee-487c185a9e90; trace=019c107c-da30-7c53-b4dc-ad8f9ca4d9e6,id=019c107c-da30-7c53-b4dc-ad8f9ca4d9e6; trace=019c107d-274e-7570-b641-752bd418ad44,id=019c107d-274e-7570-b641-752bd418ad44; trace=019c107d-274e-7570-b641-752bd418ad44,id=019c107d-274f-7941-b57d-309280a4a94a; trace=019c107d-274e-7570-b641-752bd418ad44,id=019c107d-274f-7941-b57d-309280a4a94a; trace=019c107d-274e-7570-b641-752bd418ad44,id=019c107d-2750-7a30-8f89-0f3b1e6b947c\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Agent: Представьте простую схему в PostgreSQL: users(id PK, name), orders(id PK, user_id FK → users.id, amount numeric, created_at timestamp). Напишите SQL‑запрос, который возвращает для всех пользователей (включая тех, у кого нет заказов) сумму amount по их заказам за последний календарный месяц, ноль для пользователей без заказов, и отсортирован по сумме по убыванию. Дополнительно опишите, какие индексы вы бы создали для ускорения такого запроса и какие шаги вы бы сделали, чтобы профилировать и оптимизировать его (например, использование EXPLAIN, возможные переписывания запроса).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=019c107d-274e-7570-b641-752bd418ad44,id=019c107d-2750-7a30-8f89-0f3b1e6b947c; trace=019c107d-274e-7570-b641-752bd418ad44,id=019c107d-654b-75d3-bc29-b82cf3ebb821; trace=019c107d-274e-7570-b641-752bd418ad44,id=019c107d-654b-75d3-bc29-b82cf3ebb821; trace=019c107d-274e-7570-b641-752bd418ad44,id=019c107d-274e-7570-b641-752bd418ad44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Я не знаю\n",
            "\n",
            "User: Я не знаю\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=019c107f-4392-7b11-9f60-69856904f3ba,id=019c107f-4392-7b11-9f60-69856904f3ba; trace=019c107f-4392-7b11-9f60-69856904f3ba,id=019c107f-4394-73d1-b2c9-f8b1669b1cdc; trace=019c107f-4392-7b11-9f60-69856904f3ba,id=019c107f-4394-73d1-b2c9-f8b1669b1cdc; trace=019c107f-4392-7b11-9f60-69856904f3ba,id=019c107f-4395-70e1-8f37-d9813c3959ec\n",
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=019c107f-4392-7b11-9f60-69856904f3ba,id=019c107f-4395-70e1-8f37-d9813c3959ec; trace=019c107f-4392-7b11-9f60-69856904f3ba,id=019c107f-7e5d-73b3-aeae-1909e26df259; trace=019c107f-4392-7b11-9f60-69856904f3ba,id=019c107f-7e5d-73b3-aeae-1909e26df259; trace=019c107f-4392-7b11-9f60-69856904f3ba,id=019c107f-4392-7b11-9f60-69856904f3ba; trace=019c107f-7e60-7640-a51c-ffebfb9e4256,id=019c107f-7e60-7640-a51c-ffebfb9e4256; trace=019c107f-7e60-7640-a51c-ffebfb9e4256,id=019c107f-7e61-77c3-a7fd-d441bbbb66d6; trace=019c107f-7e60-7640-a51c-ffebfb9e4256,id=019c107f-7e61-77c3-a7fd-d441bbbb66d6; trace=019c107f-7e60-7640-a51c-ffebfb9e4256,id=019c107f-7e62-70a0-a11b-f4656ea9c8a6\n",
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=019c107f-7e60-7640-a51c-ffebfb9e4256,id=019c107f-7e62-70a0-a11b-f4656ea9c8a6; trace=019c107f-7e60-7640-a51c-ffebfb9e4256,id=019c107f-d43d-7db0-b350-0c5d43c510a4; trace=019c107f-7e60-7640-a51c-ffebfb9e4256,id=019c107f-d43d-7db0-b350-0c5d43c510a4; trace=019c107f-7e60-7640-a51c-ffebfb9e4256,id=019c107f-7e60-7640-a51c-ffebfb9e4256; trace=019c107f-d441-7052-a4aa-01ea0c5da9c8,id=019c107f-d441-7052-a4aa-01ea0c5da9c8; trace=019c107f-d441-7052-a4aa-01ea0c5da9c8,id=019c107f-d442-7253-998e-ea64815dcc11; trace=019c107f-d441-7052-a4aa-01ea0c5da9c8,id=019c107f-d442-7253-998e-ea64815dcc11; trace=019c107f-d441-7052-a4aa-01ea0c5da9c8,id=019c107f-d443-7a91-b94a-02a1be4f41fa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Agent: Опишите и покажите (псевдокодом или примером кода) реализацию в Django простого CRUD-эндпоинта для заказов: укажите модель Order (поля), простой сериализатор/форму с валидацией (например, amount > 0), представления для создания заказа (POST /orders/) и получения списка заказов (GET /orders/?user_id=...), как вы обрабатываете ошибки и транзакции при создании, и какие HTTP-статусы возвращаете.\n",
            "\n",
            "User: Честно говоря, я читал на Хабре, что в Python 4.0 циклы for уберут и заменят на нейронные связи, поэтому я их не учу.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=019c107f-d441-7052-a4aa-01ea0c5da9c8,id=019c107f-d443-7a91-b94a-02a1be4f41fa; trace=019c107f-d441-7052-a4aa-01ea0c5da9c8,id=019c107f-f2fb-70e3-a8b4-eb53e68455a7; trace=019c107f-d441-7052-a4aa-01ea0c5da9c8,id=019c107f-f2fb-70e3-a8b4-eb53e68455a7; trace=019c107f-d441-7052-a4aa-01ea0c5da9c8,id=019c107f-d441-7052-a4aa-01ea0c5da9c8; trace=019c107f-f2ff-7310-b15d-b5a0b1cde169,id=019c107f-f2ff-7310-b15d-b5a0b1cde169; trace=019c107f-f2ff-7310-b15d-b5a0b1cde169,id=019c107f-f300-7930-b9c0-97651d2097e5; trace=019c107f-f2ff-7310-b15d-b5a0b1cde169,id=019c107f-f300-7930-b9c0-97651d2097e5; trace=019c107f-f2ff-7310-b15d-b5a0b1cde169,id=019c107f-f301-7150-afa6-165254400ef8\n",
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=019c107f-f2ff-7310-b15d-b5a0b1cde169,id=019c107f-f301-7150-afa6-165254400ef8; trace=019c107f-f2ff-7310-b15d-b5a0b1cde169,id=019c1080-0a85-75c1-8d73-1263cd6a8b7e; trace=019c107f-f2ff-7310-b15d-b5a0b1cde169,id=019c1080-0a85-75c1-8d73-1263cd6a8b7e; trace=019c107f-f2ff-7310-b15d-b5a0b1cde169,id=019c107f-f2ff-7310-b15d-b5a0b1cde169; trace=019c1080-0a89-7050-88ff-271dd604c649,id=019c1080-0a89-7050-88ff-271dd604c649; trace=019c1080-0a89-7050-88ff-271dd604c649,id=019c1080-0a8a-7fa1-a99e-4cd06b4c66e5; trace=019c1080-0a89-7050-88ff-271dd604c649,id=019c1080-0a8a-7fa1-a99e-4cd06b4c66e5; trace=019c1080-0a89-7050-88ff-271dd604c649,id=019c1080-0a8b-7c71-ba7e-41796ac54080\n"
          ]
        }
      ]
    }
  ]
}